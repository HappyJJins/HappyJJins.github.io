[{"categories":["지식 쌓기"],"contents":"감정대리인 티비 프로그램 중에 연애 프로그램, 데이트 하는 프로그램이 예전에도 인기였고 요즘도 인기다. 예전에 있던 프로그램과 요즘 프로그램을 비교해 보겠다. 예전 프로그램에는 ‘사랑의 스튜디오’, ‘짝’, ‘천생연분’, ‘장미의 전쟁’ 같은 것들이 있었다. 근래 2~3년 동안에는 ‘하트 시그널’, ‘로맨스 패키지’, ‘선다방’ 등의 많은 연애 프로그램이 있었다. 과거 프로그램과 최근 프로그램의 차이가 무엇일까. 요즘 프로그램은 ‘액자형 프로그램’이라고 불린다. 쉽게 말해 그들이 연애하는 모습을 스튜디오에 나와 있는 패널들이 먼저 보면서 중간에서 설명을 해주고 우리들은 그들을 거쳐 한 번 더 보게 된다. 먼저 본 사람들이 나와 해설하는 것을 보면 참 재밌다. 예를 들어 패널들이 심리 해설가 같은 역할을 한다. 남자 출연자가 여자 출연자를 처음 만나 인사하는 자리에서 눈을 본다. 이 때 한 패널이 남자는 호감 가는 여자와의 첫 만남 때 눈을 보지 않는데 저 남자는 눈을 보고 있으니 저 둘은 프로그램이 끝날 때까지 커플이 되지 않는다고 말한다. 시청자는 그 말대로 생각하고 볼 것이다. 시청자는 프로그램을 보고 스스로 평가하기 보다는 중간에서 감정을 대리해 해설해주는 사람을 거친다는 것이 오늘 키워드의 핵심이다.\n연애 경험이 많거나 심리 전문가인 패널들의 생각을 듣고 그럴 수 있겠다는 공감은 할 수 있을지 몰라도 굳이 남의 설명을 들어야만 알 수 있는 것인가 하는 씁쓸한 기분이 들기도 하다.\n생각보다 일상에서 감정을 대리하는 사례가 많다. 인터넷 기사를 볼 때 댓글을 보는 두 가지 유형이 있다. 기사를 다 읽고 댓글을 보는 유형, 댓글을 먼저 읽고 기사를 보는 유형으로 나눌 수 있다. 후자에 해당하는 많은 사람들이 그 기사에 대한 찬반 논쟁이 벌어지는 댓글을 먼저 읽고 어느 한 쪽에 감정이입을 한 상태에서 기사를 읽는다. 훨씬 자극적이고 재밌게 느껴지기 때문이다. 이런 사람들은 다른 사람들의 감정을 먼저 대리하고 본인의 감정을 투여한다. 그만큼 사람들이 자신의 감정에 확신이 없거나 잘 모른다고 해석할 수 있다. 어떠한 상황을 스스로 평가하지 않고 남들의 대리 형태를 통해서 해석하고 싶어 하는 니즈가 있는 것이다.\n이모티콘 이야말로 우리의 감정을 대리하는 극단적인 사례다. 많은 사람들이 메신저를 보낼 때 문장 끝에 무언가를 붙인다. ‘\u0026hellip;’을 붙이거나, ‘~’를 붙이기도 한다. 절대 붙이면 안 되는 것이 있다. ‘ㅋ’다. ‘ㅋ’을 절대 한 개 붙이면 안 된다. ‘ㅋ’ 한 개는 비웃음을 뜻한다고 한다. 요즘 젊은 친구들은 ‘ㅋ’을 두 줄까지는 적어줘야 웃기다, 재밌다는 표현이 된다고 한다. 오해를 방지하려면 차라리 감정을 대리해주는 이모티콘을 쓰는 것이 더 효율적이다.\n자신의 감정을 잘 모르고, 감정을 표현하는 것에 서툴고, 주저하고, 겁내는 것이 좋게 만은 보이지 않는다. 어떤 이유 때문에 감정을 대리하는 것일까. 젊은 세대들은 과잉 감정을 부담스러워한다. 아주 슬프거나 아주 기쁜 것보다는 중간 단계의 감정을 선호한다. 예를 들어 음악 방송에서 뮤지션들이 1등을 하면 우는 것을 보고 힘들었겠다고 생각하기보다는 ‘즙 짠다’는 댓글을 다는 사람이 있다고 한다. 우는 등의 격렬한 감정 표현을 하는 것에 대해 불편하게 느낀다. 왜 이렇게 됐을까.\n요즘 젊은 세대들을 **‘컬링 세대’**라고 부른다. 부모가 자녀들이 성장하는 과정에서 좋은 것만 보고 자랐으면 좋겠다는 마음에 앞에서 빗자루질을 해준다. 친구와 싸우거나, 선생님에게 혼이 나도 엄마가 대신 해결해 준다. 따라서 좋은 감정만 느끼고 성장한 젊은 사람들이 많다.\n이것을 **‘감정의 맥도날드화’**라고 부른다. 적정 수준의 감정 상태, 맥도날드 앞에 있는 아저씨의 미소정도의 감정 상태에만 편안함을 느끼는 것이다.\n마케팅으로 활용하려면 어떻게 해야 할까.\n**‘감정 큐레이션’**이라는 상품군들이 뜨고 있다. 최신 인기 100곡 보다는 ‘슬플 때 듣고 싶은 노래’, ‘봄바람 좋은 날’과 같은 플레이리스트를 선호한다. 이와 같이 감정 상태를 큐레이션 해주는 방식의 카테고리들이 성장하는 것도 이런 현상과 일맥상통한다고 볼 수 있다.\n요즘 편의점이나 마트에 파는 휴지나 음료에 ‘힘내세요’와 같은 문구가 적혀있다. 우리의 감정을 상품을 통해 대리표현 해주는 케이스라고 볼 수 있다.\n","permalink":"https://HappyJJins.github.io/blog/post5/","tags":["감정대리인","하트시그널","액자형 프로그램","컬링세대","감정의 맥도날드화","감정 큐레이션"],"title":"감정대리인"},{"categories":["Data analysis _Python"],"contents":"Movies Recommender System Package를 설치하는 환경폴더(my_env)를 따로 만들어 실행할 때마다 설치할 필요 없도록 설정했습니다. 이와 관련한 포스팅은 다음에 하겠습니다.\nimport os, sys from google.colab import drive drive.mount(\u0026#39;/content/drive\u0026#39;) my_path = \u0026#39;/content/notebooks\u0026#39; os.symlink(\u0026#39;/content/drive/My Drive/Colab Notebooks/my_env\u0026#39;, my_path) sys.path.insert(0, my_path) !pip install scikit-surprise %matplotlib inline import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from scipy import stats from ast import literal_eval from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer from sklearn.metrics.pairwise import linear_kernel, cosine_similarity from nltk.stem.snowball import SnowballStemmer from nltk.stem.wordnet import WordNetLemmatizer from nltk.corpus import wordnet from surprise import Reader, Dataset, SVD, accuracy from surprise.model_selection import cross_validate from surprise.model_selection import KFold import warnings; warnings.simplefilter(\u0026#39;ignore\u0026#39;) Simple Recommender 영화에 대한 다양한 정보들이 담긴 \u0026lsquo;movies_metadata.csv\u0026rsquo; 파일을 불러와 md 변수에 저장합니다\nmd = pd.read_csv(\u0026#39;/content/drive/My Drive/Colab Notebooks/MovieRcmmd/data/data/movies_metadata.csv\u0026#39;) md.head() \u0026lsquo;genres\u0026rsquo;(장르) 컬럼의 각 행들의 타입이 dict. 장르 id와 이름으로 구성.\n{\u0026lsquo;id\u0026rsquo;: \u0026hellip; , \u0026lsquo;name\u0026rsquo;: \u0026lsquo;\u0026hellip;'},{\u0026lsquo;id\u0026rsquo;: \u0026hellip; ,\u0026lsquo;name\u0026rsquo; : \u0026lsquo;\u0026hellip;'}, ,,,\nmd[\u0026#39;genres\u0026#39;][0] print(type(md[\u0026#39;genres\u0026#39;][0])) # \u0026#34;[{dict}, {dict},] \u0026lt;- dict들이 str으로 묶여있음 md[\u0026#39;genres\u0026#39;].apply(literal_eval) # [{dict},{dict}] \u0026lt;- str을 list로 바꿔줌 #각 영화 장르의 이름을 추출해 list 시켜줌 md[\u0026#39;genres\u0026#39;] = md[\u0026#39;genres\u0026#39;].fillna(\u0026#39;[]\u0026#39;).apply(literal_eval).apply(lambda x : [i[\u0026#39;name\u0026#39;] for i in x] if isinstance(x, list) else []) md[\u0026#39;genres\u0026#39;] # eval(expression, globals=None, locals=none), 내장함수 # expression 인자에 string 값을 넣으면 해당 값을 그대로 실행하여 결과를 출력 expr = \u0026#34;10 + 10\u0026#34; print(type(expr)) print(eval(expr)) #ast.literal_eval(node_or_string), AST module에서 제공하는 함수 #문자 그대로 evaluate 실행하는 함수 import ast str_dict = \u0026#34;{\u0026#39;a\u0026#39;:3,\u0026#39;b\u0026#39;:5}\u0026#34; print(type(str_dict)) convert_dict= ast.literal_eval(str_dict) print(convert_dict[\u0026#39;a\u0026#39;]) print(convert_dict[\u0026#39;b\u0026#39;]) #literal_eval은 ValueError를 발생시킬 수 있다. ast.literal_eval(\u0026#34;10*2\u0026#34;) #literal_eval은 python의 기본 자료형 정도만 evaluate가 가능하도록 지원. eval과 비교해 훨씬 엄격하기 때문에 결과적으로 안전을 보장. type(md[\u0026#39;vote_count\u0026#39;][0]) #vote_count의 타입이 float vote_counts = md[md[\u0026#39;vote_count\u0026#39;].notnull()][\u0026#39;vote_count\u0026#39;].astype(\u0026#39;int\u0026#39;) #float을 int로 vote_averages = md[md[\u0026#39;vote_average\u0026#39;].notnull()][\u0026#39;vote_average\u0026#39;].astype(\u0026#39;int\u0026#39;) C = vote_averages.mean() C m = vote_counts.quantile(0.95) m md[\u0026#39;year\u0026#39;] = pd.to_datetime(md[\u0026#39;release_date\u0026#39;], errors=\u0026#39;coerce\u0026#39;).apply(lambda x : str(x).split(\u0026#39;-\u0026#39;)[0] if x != np.nan else np.nan) # error=\u0026#39;coerce\u0026#39; # 날짜로 된 문자열이 아니라 문자로 된 문자열인 경우 파싱할 수 없다는 ValueError가 발생하는 것을 방지하기 위해 \u0026#39;coerce\u0026#39;옵션 추가 # 파싱할 수 없는 문자열은 \u0026#39;NaN\u0026#39;으로 강제 변환 md[\u0026#39;year\u0026#39;] qualified = md[(md[\u0026#39;vote_count\u0026#39;] \u0026gt;= m) \u0026amp; (md[\u0026#39;vote_count\u0026#39;].notnull()) \u0026amp; (md[\u0026#39;vote_average\u0026#39;].notnull())][[\u0026#39;title\u0026#39;, \u0026#39;year\u0026#39;, \u0026#39;vote_count\u0026#39;, \u0026#39;vote_average\u0026#39;, \u0026#39;popularity\u0026#39;, \u0026#39;genres\u0026#39;]] qualified[\u0026#39;vote_count\u0026#39;] = qualified[\u0026#39;vote_count\u0026#39;].astype(\u0026#39;int\u0026#39;) qualified[\u0026#39;vote_average\u0026#39;] = qualified[\u0026#39;vote_average\u0026#39;].astype(\u0026#39;int\u0026#39;) qualified.shape #(행 수, 열 수) def weighted_rating(x): v = x[\u0026#39;vote_count\u0026#39;] R = x[\u0026#39;vote_average\u0026#39;] return (v/(v+m)*R)+(m/(m+v)*C) # Weighted Rating(WR) # v is the number of votes for the movie # m is the minimum votes required to be listed in the chart # R is the average rating of the movie # C is the mean vote across the whole report qualified[\u0026#39;wr\u0026#39;] = qualified.apply(weighted_rating, axis=1) qualified = qualified.sort_values(\u0026#39;wr\u0026#39;, ascending=False).head(250) qualified[\u0026#39;wr\u0026#39;] qualified.head(15) md.apply(lambda x : pd.Series(x[\u0026#39;genres\u0026#39;]), axis=1) md.apply(lambda x : pd.Series(x[\u0026#39;genres\u0026#39;]), axis=1).stack() s = md.apply(lambda x : pd.Series(x[\u0026#39;genres\u0026#39;]), axis=1).stack().reset_index(level=1, drop=True) # 인덱스 1단계 제거(첫번째 열) s s.name = \u0026#39;genre\u0026#39; gen_md = md.drop(\u0026#39;genres\u0026#39;, axis = 1).join(s) gen_md def build_chart(genre, percentile=0.86): df = gen_md[gen_md[\u0026#39;genre\u0026#39;] == genre] vote_counts = df[df[\u0026#39;vote_count\u0026#39;].notnull()][\u0026#39;vote_count\u0026#39;].astype(\u0026#39;int\u0026#39;) vote_averages = df[df[\u0026#39;vote_average\u0026#39;].notnull()][\u0026#39;vote_average\u0026#39;].astype(\u0026#39;int\u0026#39;) C = vote_averages.mean() m = vote_counts.quantile(percentile) # 여러 개의 칼럼 데이터 추추 시 대괄호 두번[[]] qualified = df[(df[\u0026#39;vote_count\u0026#39;] \u0026gt;= m) \u0026amp; (df[\u0026#39;vote_count\u0026#39;].notnull()) \u0026amp; (df[\u0026#39;vote_average\u0026#39;].notnull())][[\u0026#39;title\u0026#39;, \u0026#39;year\u0026#39;, \u0026#39;vote_count\u0026#39;, \u0026#39;vote_average\u0026#39;, \u0026#39;popularity\u0026#39;]] qualified[\u0026#39;vote_count\u0026#39;] = qualified[\u0026#39;vote_count\u0026#39;].astype(\u0026#39;int\u0026#39;) qualified[\u0026#39;vote_average\u0026#39;] = qualified[\u0026#39;vote_average\u0026#39;].astype(\u0026#39;int\u0026#39;) qualified[\u0026#39;wr\u0026#39;] = qualified.apply(lambda x: (x[\u0026#39;vote_count\u0026#39;]/(x[\u0026#39;vote_count\u0026#39;]+m)*x[\u0026#39;vote_average\u0026#39;])+(m/(m+x[\u0026#39;vote_count\u0026#39;])*C), axis=1) qualified = qualified.sort_values(\u0026#39;wr\u0026#39;, ascending=False).head(250) return qualified Top Romance Movies\nbuild_chart(\u0026#39;Romance\u0026#39;).head(15) links_small = pd.read_csv(\u0026#39;/content/drive/My Drive/Colab Notebooks/MovieRcmmd/data/data/links_small.csv\u0026#39;) links_small # imdbid : internet movie data base # tmdbid : the movie data base links_small = links_small[links_small[\u0026#39;tmdbId\u0026#39;].notnull()][\u0026#39;tmdbId\u0026#39;].astype(\u0026#39;int\u0026#39;) links_small # 여러 개의 행 추출 시 대괄호 두 번 [[]] md.loc[[19730, 29503, 35587]] #id가 날짜 형식 md = md.drop([19730, 29503, 35587]) md[\u0026#39;id\u0026#39;] = md[\u0026#39;id\u0026#39;].astype(\u0026#39;int\u0026#39;) smd = md[md[\u0026#39;id\u0026#39;].isin(links_small)] smd.shape smd.loc[1][\u0026#39;title\u0026#39;] smd.loc[1][\u0026#39;overview\u0026#39;] # overview : 개요 smd.loc[1][\u0026#39;tagline\u0026#39;] # tagline : 광고 구호, 슬로건 smd[\u0026#39;tagline\u0026#39;] = smd[\u0026#39;tagline\u0026#39;].fillna(\u0026#39;\u0026#39;) smd[\u0026#39;description\u0026#39;] = smd[\u0026#39;overview\u0026#39;] + smd[\u0026#39;tagline\u0026#39;] smd[\u0026#39;description\u0026#39;] = smd[\u0026#39;description\u0026#39;].fillna(\u0026#39;\u0026#39;) # tf-idf 인코딩 : 단어 갯수 그대로 카운트하지 않고 모든 문서에 공통적으로 들어있는 단어의 경우 문서 구별 능력이 떨어진다고 보아 가중치를 축소하는 방법 # analyzer=\u0026#39;word\u0026#39;/\u0026#39;char\u0026#39;/\u0026#39;char_wb\u0026#39;(단어 내의 문자) # ngram_range=(min_n, max_n) # min_df=[0.0,1.0]사이의 실수. 디폴트 1, 단어장에 포함되기 위한 최소 빈도 # stop_words=\u0026#39;english\u0026#39; : 영어용 스탑 워드 사용 tf = TfidfVectorizer(analyzer=\u0026#39;word\u0026#39;, ngram_range=(1,2),min_df=0, stop_words=\u0026#39;english\u0026#39;) tfidf_matrix = tf.fit_transform(smd[\u0026#39;description\u0026#39;]) tf.vocabulary_ # 단어를 인덱싱, type은 dict tfidf_matrix.shape # 9099개의 문장이 268124 토큰으로 표현됨 print(np.array(tfidf_matrix)) # (m, n) xxx : 문장 m에서의 단어 n의 tfidf값 cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix) cosine_sim[0] # 1 :첫번째 문장 자기자신과의 코사인 유사도 # 0.00680476 :첫번째 문장과 두 번째 문장과의 코사인 유사도 cosine_sim[9098] smd = smd.reset_index() titles = smd[\u0026#39;title\u0026#39;] indices = pd.Series(smd.index, index=smd[\u0026#39;title\u0026#39;]) smd indices def get_recommendations(title): idx = indices[title] #해당 title 인덱스 sim_scores = list(enumerate(cosine_sim[idx])) #해당 title 과 다른 영화와의 유사도 인덱싱 # (1, 0.00282712634654008) :해당 title과 1번째 영화와의 유사도가 0.002827126346... sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) #유사도를 기준으로 내림차순으로 정렬 sim_scores = sim_scores[1:31] #자기자신과의 유사도(1)을 제외하고, 유사도가 가장 높은 것부터 30개 추출 movie_indices = [i[0] for i in sim_scores] #유사도 상위30개 영화에 대한 인덱스 return titles.iloc[movie_indices] list(enumerate(cosine_sim[indices[\u0026#39;The Godfather\u0026#39;]])) sorted(list(enumerate(cosine_sim[indices[\u0026#39;The Godfather\u0026#39;]])), key=lambda x: x[1], reverse=True) get_recommendations(\u0026#39;The Godfather\u0026#39;).head(10) get_recommendations(\u0026#39;The Dark Knight\u0026#39;).head(10) Metadata Based Recommender\ncredits = pd.read_csv(\u0026#39;/content/drive/My Drive/Colab Notebooks/MovieRcmmd/data/data/credits.csv\u0026#39;) keywords = pd.read_csv(\u0026#39;/content/drive/My Drive/Colab Notebooks/MovieRcmmd/data/data/keywords.csv\u0026#39;) keywords[\u0026#39;id\u0026#39;] = keywords[\u0026#39;id\u0026#39;].astype(\u0026#39;int\u0026#39;) credits[\u0026#39;id\u0026#39;] = credits[\u0026#39;id\u0026#39;].astype(\u0026#39;int\u0026#39;) md[\u0026#39;id\u0026#39;] = md[\u0026#39;id\u0026#39;].astype(\u0026#39;int\u0026#39;) keywords.head() # 각 영화의 keywords 정리 credits.head() # cast :주연들에 대한 설명(character, name, gender,,) # crew : 감독, 작가 등에 대한 설명 md.shape md = md.merge(credits, on=\u0026#39;id\u0026#39;) md = md.merge(keywords, on=\u0026#39;id\u0026#39;) md.head() smd = md[md[\u0026#39;id\u0026#39;].isin(links_small)] smd.shape smd[\u0026#39;cast\u0026#39;] = smd[\u0026#39;cast\u0026#39;].apply(literal_eval) smd[\u0026#39;crew\u0026#39;] = smd[\u0026#39;crew\u0026#39;].apply(literal_eval) smd[\u0026#39;keywords\u0026#39;] = smd[\u0026#39;keywords\u0026#39;].apply(literal_eval) smd[\u0026#39;cast_size\u0026#39;] = smd[\u0026#39;cast\u0026#39;].apply(lambda x: len(x)) smd[\u0026#39;crew_size\u0026#39;] = smd[\u0026#39;crew\u0026#39;].apply(lambda x: len(x)) def get_director(x): for i in x: if i[\u0026#39;job\u0026#39;] == \u0026#39;Director\u0026#39;: return i[\u0026#39;name\u0026#39;] return np.nan smd[\u0026#39;director\u0026#39;] = smd[\u0026#39;crew\u0026#39;].apply(get_director) smd[\u0026#39;cast\u0026#39;] = smd[\u0026#39;cast\u0026#39;].apply(lambda x: [i[\u0026#39;name\u0026#39;] for i in x] if isinstance(x, list) else []) smd[\u0026#39;cast\u0026#39;] = smd[\u0026#39;cast\u0026#39;].apply(lambda x: x[:3] if len(x) \u0026gt;=3 else x) #주연이름 세자리까지만 smd[\u0026#39;keywords\u0026#39;] = smd[\u0026#39;keywords\u0026#39;].apply(lambda x: [i[\u0026#39;name\u0026#39;] for i in x] if isinstance(x, list) else []) smd[\u0026#39;cast\u0026#39;] = smd[\u0026#39;cast\u0026#39;].apply(lambda x: [str.lower(i.replace(\u0026#34; \u0026#34;,\u0026#34;\u0026#34;)) for i in x]) # 주연이름 붙여쓰기, 소문자로 smd[\u0026#39;cast\u0026#39;].head() smd[\u0026#39;director\u0026#39;] = smd[\u0026#39;director\u0026#39;].astype(\u0026#39;str\u0026#39;).apply(lambda x: str.lower(x.replace(\u0026#34; \u0026#34;, \u0026#34;\u0026#34;))) smd[\u0026#39;director\u0026#39;] = smd[\u0026#39;director\u0026#39;].apply(lambda x: [x, x, x]) smd[\u0026#39;director\u0026#39;] Keywords\ns = smd.apply(lambda x: pd.Series(x[\u0026#39;keywords\u0026#39;]), axis=1).stack().reset_index(level=1, drop=True) s.name = \u0026#39;keyword\u0026#39; s s = s.value_counts() s[:5] s = s[s \u0026gt; 1] stemmer = SnowballStemmer(\u0026#39;english\u0026#39;) #접사 제거, 명사 추출 stemmer.stem(\u0026#39;dogs\u0026#39;) def filter_keywords(x): words = [] for i in x: if i in s: words.append(i) return words smd[\u0026#39;keywords\u0026#39;] = smd[\u0026#39;keywords\u0026#39;].apply(filter_keywords) smd[\u0026#39;keywords\u0026#39;] smd[\u0026#39;keywords\u0026#39;] = smd[\u0026#39;keywords\u0026#39;].apply(lambda x: [stemmer.stem(i) for i in x]) smd[\u0026#39;keywords\u0026#39;] = smd[\u0026#39;keywords\u0026#39;].apply(lambda x: [str.lower(i.replace(\u0026#34; \u0026#34;, \u0026#34;\u0026#34;)) for i in x]) smd[\u0026#39;keywords\u0026#39;] smd[\u0026#39;soup\u0026#39;] = smd[\u0026#39;keywords\u0026#39;] + smd[\u0026#39;cast\u0026#39;] + smd[\u0026#39;director\u0026#39;] + smd[\u0026#39;genres\u0026#39;] smd[\u0026#39;soup\u0026#39;] = smd[\u0026#39;soup\u0026#39;].apply(lambda x: \u0026#39; \u0026#39;.join(x)) smd[\u0026#39;soup\u0026#39;] count = CountVectorizer(analyzer=\u0026#39;word\u0026#39;, ngram_range=(1, 2), min_df=0, stop_words=\u0026#39;english\u0026#39;) count_matrix = count.fit_transform(smd[\u0026#39;soup\u0026#39;]) count_matrix.shape # 9212개의 문장이 100304 토큰으로 표현 print(np.array(count_matrix)) cosine_sim = cosine_similarity(count_matrix, count_matrix) cosine_sim smd = smd.reset_index() titles = smd[\u0026#39;title\u0026#39;] indices = pd.Series(smd.index, index=smd[\u0026#39;title\u0026#39;]) indices get_recommendations(\u0026#39;The Dark Knight\u0026#39;).head(10) get_recommendations(\u0026#39;Mean Girls\u0026#39;).head(10) Popularity and Ratings\ndef improved_recommendations(title): idx = indices[title] sim_scores = list(enumerate(cosine_sim[idx])) sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) sim_scores = sim_scores[1:26] movie_indices = [i[0] for i in sim_scores] movies = smd.iloc[movie_indices][[\u0026#39;title\u0026#39;, \u0026#39;vote_count\u0026#39;, \u0026#39;vote_average\u0026#39;, \u0026#39;year\u0026#39;]] vote_counts = movies[movies[\u0026#39;vote_count\u0026#39;].notnull()][\u0026#39;vote_count\u0026#39;].astype(\u0026#39;int\u0026#39;) vote_averages = movies[movies[\u0026#39;vote_average\u0026#39;].notnull()][\u0026#39;vote_average\u0026#39;].astype(\u0026#39;int\u0026#39;) C = vote_averages.mean() m = vote_counts.quantile(0.60) qualified = movies[(movies[\u0026#39;vote_count\u0026#39;] \u0026gt;= m) \u0026amp; (movies[\u0026#39;vote_count\u0026#39;].notnull()) \u0026amp; (movies[\u0026#39;vote_average\u0026#39;].notnull())] qualified[\u0026#39;vote_count\u0026#39;] = qualified[\u0026#39;vote_count\u0026#39;].astype(\u0026#39;int\u0026#39;) qualified[\u0026#39;vote_average\u0026#39;] = qualified[\u0026#39;vote_average\u0026#39;].astype(\u0026#39;int\u0026#39;) qualified[\u0026#39;wr\u0026#39;] = qualified.apply(weighted_rating, axis=1) qualified = qualified.sort_values(\u0026#39;wr\u0026#39;, ascending=False).head(10) return qualified improved_recommendations(\u0026#39;The Dark Knight\u0026#39;) improved_recommendations(\u0026#39;Mean Girls\u0026#39;) Collaborative Filtering\nreader = Reader() ratings = pd.read_csv(\u0026#39;/content/drive/My Drive/Colab Notebooks/MovieRcmmd/data/data/ratings_small.csv\u0026#39;) ratings.head() data = Dataset.load_from_df(ratings[[\u0026#39;userId\u0026#39;, \u0026#39;movieId\u0026#39;, \u0026#39;rating\u0026#39;]], reader) #데이터셋 로딩 svd = SVD() # SVD :특이값 분해 알고리즘 # 데이터를 5개의 부분집합{x1, x2, ,,, , x5}으로 나눈다. # 5개의 부분집합 중 하나의 검증용 데이터셋(test_set)를 제외한 나머지 데이터셋을 학습용 데이터(train_set)로 사용하여 회귀분석 모형을 만들고 test_set으로 검증 # 5회 반복 cross_validate(svd, data, measures=[\u0026#39;RMSE\u0026#39;, \u0026#39;MAE\u0026#39;], cv=5, verbose=True) # kf = KFold(n_splits = 5) # for train_set, test_set in kf.split(data): # svd.fit(train_set) # predictions = svd.test(test_set) # RMSE = accuracy.rmse(predictions, verbose=True) # MAE = accuracy.mae(predictions, verbose=True) print(svd.predict(1, 302)) # 1번 user가 302번 movie에 줬을 점수 추정치가 2.94 Hybrid Recommender\n Input :User ID and the Title of a Movie Output :Similar movies sorted on the basis of expected ratins by that particular user  def convert_int(x): try: return int(x) except: return np.nan id_map = pd.read_csv(\u0026#39;/content/drive/My Drive/Colab Notebooks/MovieRcmmd/data/data/links_small.csv\u0026#39;)[[\u0026#39;movieId\u0026#39;, \u0026#39;tmdbId\u0026#39;]] id_map[\u0026#39;tmdbId\u0026#39;] = id_map[\u0026#39;tmdbId\u0026#39;].apply(convert_int) id_map.columns = [\u0026#39;movieId\u0026#39;, \u0026#39;id\u0026#39;] id_map = id_map.merge(smd[[\u0026#39;title\u0026#39;, \u0026#39;id\u0026#39;]], on=\u0026#39;id\u0026#39;).set_index(\u0026#39;title\u0026#39;) id_mapindices indices_map = id_map.set_index(\u0026#39;id\u0026#39;) indices_map def hybrid(userId, title): idx = indices[title] tmdbId = id_map.loc[title][\u0026#39;id\u0026#39;] movie_id = id_map.loc[title][\u0026#39;movieId\u0026#39;] sim_scores = list(enumerate(cosine_sim[int(idx)])) sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) sim_scores = sim_scores[1:26] movie_indices = [i[0] for i in sim_scores] movies = smd.iloc[movie_indices][[\u0026#39;title\u0026#39;, \u0026#39;vote_count\u0026#39;, \u0026#39;vote_average\u0026#39;, \u0026#39;year\u0026#39;, \u0026#39;id\u0026#39;]] movies[\u0026#39;est\u0026#39;] = movies[\u0026#39;id\u0026#39;].apply(lambda x: svd.predict(userId, indices_map.loc[x][\u0026#39;movieId\u0026#39;]).est) movies = movies.sort_values(\u0026#39;est\u0026#39;, ascending=False) return movies.head(10) hybrid(1, \u0026#39;Avatar\u0026#39;) # 1번 유저가 선택한 영화 \u0026#39;Avatar\u0026#39;와 유사도가 높은 25개의 영화를 추출해 젔을 법한 점수(rating) 추정 # rating est 상위 10개 추출 hybrid(500, \u0026#39;Avatar\u0026#39;) ","permalink":"https://HappyJJins.github.io/blog/post4/","tags":["추천알고리즘","영화추천알고리즘","Python","kaggle"],"title":"영화 추천 알고리즘"},{"categories":["지식 쌓기"],"contents":"IT가 양날의 검이 되는 세상 IT가 양날의 검이 되는 세상이다. 원격근무와 화상회의가 코로나 시대에 좋을 줄만 알았더니 무조건 그렇지도 않다. ‘언텍트’가 더 이상 어색하지 않을 정도로 익숙해졌다. 비대면 서비스가 활성화되고 있다. 그 중에서도 많이 활성화되고 있는 것이 화상회의, 화상교육이다. 코로나19가 길어지다 보니 면접도 화상면접으로 바뀌고 있다. 초기에는 몇몇 IT 기업들만 그런 줄 알았는데 상반기에 공채들을 봤을 때 화상면접으로 많이 바뀌고 있는 추세다. 사람들을 뽑을 때도 언텍트 기술, 화상회의 서비스를 사용하고 있다. 미국에서는 해고도 화상으로 하고 있다. 실리콘 밸리 IT기업에서 상당히 이루어지고 있다. 유동성이 심하다보니 한 번에 대량 해고를 할 때 화상회의 솔루션을 쓰는 것이다. 예를 들어 한 전기 스쿠터 제조사 경우 전체 직원의 30%인 400명을 한 번에 해고했다. 해고 당시 화상회의 솔루션을 활용했다. 이것에 대해 너무하는 것 아니냐는 말에 사장은 이런 얘기를 했다. ‘우리는 조금 더 인간적으로 하려고 비디오는 껐다.’고 말이다. 다른 벤처, 스타트업 같은 경우도 직원을 350명 해고할 때 ‘줌’을 썼다고 한다. 그 CEO는 화상회의를 통해 직원을 해고하는 것이 가장 끔찍한 방식일지도 모르지만 다른 선택이 없었다고 했다. 코로나 때문에 고용시장이 상당히 불안정한 상태다. 이런 식으로 해고하는 기업이 많아질 수밖에 없다는 얘기처럼 들린다. 특히 국내는 고용에 대해 보호받는 부분이 있는데 미국은 계약이나 해고가 우리나라에 비해 자유롭다보니 대규모의 채용이나 해고도 원격으로 진행되고 있다. 그럴 수밖에 없는 이유들 중 하나는 업무 자체의 방식을 완벽하게 언텍트로 바꾸려는 곳도 있다. 지금 아마 우리나라에 몇몇 대학들도 학기 전체를 원격으로 한다. 그만큼이나 회사에 들어갔지만 물리적인 공간을 한 번도 못 가보고 출근과 퇴근도 한 번도 해보지 못하고 채용이 됐다가 잘리게 되는 문화도 생길 수 있다고 볼 수 있다. 화상회의 프로그램이나 앱의 관심이 높아질 수밖에 없고 많이 시작할 것이다. 코로나19로 대부분의 주식들이 하락했고, 경제상황이 좋아지지 않고 있다. 그런데 몇몇 안 되는 홀로 웃는 서비스가 있다. 바로 미국에서 가장 주목 받고 있는 화상회의 솔루션 ‘줌’이다. Z세대가 ‘Zoom’의 Z라는 얘기가 나올 정도로 화제다. 줌을 사용하는 사람들을 지칭하는 ‘Zoomer’라는 신조어도 생겼다. ‘University’ 앞에 ‘Z’를 붙여 ‘줌대학’이라는 용어도 쓰고 있다. 한편 해외 유명대학들이 이 솔루션을 활용하고 있는데 등록금 등의 문제로 학생들의 불만이 많다. 온라인 강의를 들으면서 이렇게 많은 등록금을 내야하는가에 대한 문제다. 최근 주목받는 이유는 직관적이라는 것이다. 다른 서비스들은 그 서비스를 이용하는 사람들은 모두 회원가입을 해 방을 만들어야 한다. 이 서비스는 처음 방을 만드는 사람만 가입해 함께 이용하고 싶은 사람에게 링크를 보내주는데, 그 링크만 있으면 누구나 들어갈 수 있다. 접근성이 상당히 편리하고, 모바일 앱이나 웹을 통해 이용할 수 있다. 최대 100명까지 들어갈 수 있는데, 40분을 무료로 이용할 수 있다. 이로 인해 상당히 재밌는 현상이 벌어지고 있다. 미국 실리콘밸리 화상회의 평균 시간이 30분이라 하는데, 공짜 시간 안에 하려는 것 아닌가 하는 여담이 나오고 있다. 영국의 내각 회의나 우리나라 교육부 회의에서도 이 앱을 활용하고 있다고 한다. 빛이 있으면 그림자가 있는 법이다. 누구나 접근하기 편하다는 장점을 누군가 악용하고 있다. ‘Zoom Booming’이라는 신조어가 생겼다. 줌 폭탄이 떨어진다는 뜻이다. 예를 들어 한 선생님이 방을 만들어 학생들과 화상회의를 하려는데 쉽게 링크를 보내기 위해 SNS에 게재했다. 그런데 학생이 아닌 사람들이 이 링크를 타고 방에 들어가 야한 동영상을 띄우거나 욕을 하고 나가는 것이다. 이런 현상이 실제로 미국에서 일어나고 있다. 이런 경우는 사용자가 방에 비밀번호를 걸어놓지 않은 실수일 경우가 많다. 개인정보를 너무 많이 가져간다는 단점도 있다. 아이폰을 가지고 이 앱을 썼을 때 페이스북이 이 폰에 깔려있지 않는데도 개인정보가 페이스북으로 간다는 것이 발견되기도 했다. 이 문제가 하루 만에 수정되긴 했지만 이런 식으로 예상 외로 개인정보가 다른 곳으로 흘러간다는 것이 이슈화되고 있다. 또한 화상회의 내용이 암호화돼 있지 않아 해킹에 취약하다는 단점도 있다. 줌을 계속 사용하는 것이 불안하다는 내용의 기사를 외신들이 많이 쓰고 있기도 하다. 몇몇 대기업은 보안을 위해 다른 앱으로 옮기고 있다.\n상호작용은 고객과 기업이 만나는 순간을 의미한다. 기업 입장에서는 상호작용은 기회다. 상호작용 시점에 고객의 니즈를 충족시키지 못하면 고객은 다른 방식의 서비스를 찾아가게 될 것이다. ‘Zoom’은 접근성이 편리하다는 점에서 사람들을 끌었지만 ‘보안’을 해결하지 못하면 이용자들은 더 나은 서비스를 찾아 떠나갈 것이다. 줌 이용자가 폭발적으로 늘어나고 있는 지금 이 좋은 기회를 놓치지 말자.\n","permalink":"https://HappyJJins.github.io/blog/post3/","tags":["IT의 양면","빅데이터로 보는 세상","zoom","화상회의","언텍트"],"title":"IT, 양날의 검이 되는 세상"},{"categories":["지식 쌓기"],"contents":" 초개인화 MWC(Mobile World Congress) 취소 카카오톡 서비스로 증권거래도 가능  초개인화 요즘 모임 회비를 입금하거나 단체 회식비를 나눠 낼 때 어떤 방법을 선택하는가. 물론 통장 들고, 카드 들고 은행 달려가던 시절도 있었지만 이젠 스마트폰 하나면 해결되는 시대다. 2007년 스마트폰이 세상에 처음 공개됐으니 불과 13년 전 일인데 그 시절, 아주 먼 옛날 얘기처럼 느껴진다. 이젠 굳이 계좌가 있는 전통 은행 사이트에 접속하지 않고도 스마트폰 버튼만 누르면 송금이 되는 편리한 세상이 됐다. 이렇게 IT 기업 서비스 영역, 빠르게 넓어지고 있다. 2020년 금융시장이 불어올 변화와 혁신의 바람이 기대된다.\n\u0026lt;경제퀴즈\u0026gt; 신용카드나 계좌정보를 스마트폰 앱 등에 미리 등록해 놓은 사람이 많다. 미리 등록한 지문이나 비밀번호 입력만으로도 돈을 지불할 수 있다. 이 서비스, 공인인증서와 같은 복잡한 결제절차를 걸치지 않기 때문에 편리하게 이용할 수 있다. 어려운 절차 때문에 구입을 포기하는 사례 또한 줄일 수 있다. 스마트폰을 단말기로 사용할 수 있기 때문에 스마트 페이라고도 불리는 이 서비스는 무엇일까. 간편 결제\n비즈니스할 때 혹은 개인 사업이나 기업 활동을 할 때 고객을 세그먼테이션을 잘 해야 한다. 비슷한 나이나 성별, 취향, 소득수준을 기준으로 사람들을 그룹화 하는 것을 세그먼테이션이라고 한다. 그 그룹에게 맞춰 비즈니스를 한다. 이제는 그러한 분류, 그룹화가 아니라, 1000명의 소비자가 있으면 1000개의 각기 다른 시장이 있다고 보는 것이 개인화다. 이제는 그 개인화를 더 개인화한다. 한 명의 소비자를 하나의 시장이라고 보는 것이 아니라 이 사람이 지금 어떤 상황과 맥락에 놓여있는지 등을 고려해 이 사람을 0.1명으로 보는 것이다. 이렇게 사람들을 더 개인화해서 그 상황에 맞춰주는 최적화해주는 기술, 트렌드를 초개인화라고 한다. 어떤 상황이나 설정에 따라 분류했지만 사실은 다른 선택을 할 수 있는 것이다. 예를 들어 어떤 사람이 얌전한 스타일의 옷을 좋아한다지만 해외여행을 앞두고 있을 때는 좀 더 과감한 옷을 선택할 수도 있는 것처럼 상황별로 선택이 바뀔 수 있다.\n버스정류장에 서 있으면 전광판에 버스 잔여석이 뜬다. 원리는 생각보다 단순하다. 버스를 탈 때 교통카드를 찍고 내릴 때 환승하려고 카드를 또 찍는다. 탈 때 카드를 찍은 사람에서 내릴 때 찍은 사람을 빼면 버스 잔류 인원이 계산된다. 그 숫자가 적으면 빨간 불, 많으면 초록 불이 켜진다고 한다. 사람들은 빅데이터를 복잡하게 생각하지만 ‘빼기’라는 단순한 수식으로 정보를 만들어내는 것이 최근 화두다. 여기서부터 초개인화가 시작된다.\n서울에 사당이라는 곳에서 버스를 기다리는데 사당역과 가까운 고속버스터미널에서 친구를 만날 때와 좀 더 먼 잠실에서 친구를 만날 때는 상황이 다르기 때문에 의사결정에 영향을 미칠 수 있다. 고속버스터미널까지만 간다면 다리가 아프니까 좀 더 기다리더라도 여유석이 많은 버스를 타야겠다고 생각하지만 잠실까지 간다면 여유석이 좀 없더라도 버스를 탄다. 중간에 사람들이 내리면 그 때 앉겠다고 생각한다. 나는 한 명이지만 친구를 어디서 만나느냐에 따라 필요한 정보가 달라지고 의사결정 또한 달라진다.\n핵심은 어떤 사람이 놓인 상황이나 맥락에 따라 비즈니스 하는 것을 초개인화라고 한다는 것이다. 개인화 앞에 ‘초’만 붙었을 뿐인데 개인화와는 전혀 다른 개념이다. 초개인화를 이용해서 비즈니스하려면 데이터 활용을 잘 해야 할 것이다. 이것은 모든 기업들이 하고 싶은 꿈같은 이야기일 것이다. 상당히 어렵다.\n어느 온라인 쇼핑몰에서 그 다음날 배송해주는 빠른 배송 서비스를 이용해 물건을 구입한 적이 있다. 이미 사서 물건을 받았지만 그 물건을 검색하고 구입해 받은 순간으로부터 열흘 간 국내에 있는 포털 사이트에 들어가면 그 물건 광고가 계속 떴다. 이 회사는 그 물건을 구매했다는 사실은 모르고 검색했다는 기록만 가지고 있다고 짐작했다. 아직은 우리나라 기업들이 고객들이 어떤 상황에 놓여있는지 그 맥락을 파악하기 위한 데이터 수집력이나 분석 수준이 아직은 부족하다고 느꼈다.\n그렇다면 발 빠르게 따라가 초개인화를 잘 활용하고 있는 기업은 어디일까. 초개인화를 전 세계에서 가장 잘 활용하고 있다고 평가받는 회사가 있다. **‘아마존’**이다. 특히 배송시스템에서 초개인화를 상당히 잘 활용하고 있다.\n어떤 사람은 온라인을 물건을 살 때 그 자리에서 고민을 끝낸다. 어떤 사람들은 며칠을 고민한다. 이렇게 사람마다 구매 스타일이 각기 다른데 아마존은 이를 잘 안다. 전자에 해당하는 사람이 아마존에 접속해 몇 개의 물건을 가지고 고민하면 아마존의 슈퍼컴퓨터의 알고리즘이 분석해 그 제품들 중 구매할 확률이 높은 제품들을 추려 그 고객의 집과 가장 가까운 물류센터에 미리 대기시켜 놓는다. 틀림없이 그 고객은 대기 중인 제품을 사게 된다. 구매확정이 되면 그 제품을 총알 같이 배송한다. 대신 그 고객은 구매율이 높기 때문에 할인을 잘 해주지 않는다. 정가를 거의 다 받아낸다. 정가에도 잘 사기 때문이다. 반면 후자에 해당하는 사람은 정가에 잘 사지 않는다. 할인을 해줘야 산다. 이런 사람에게는 아마존이 쿠폰을 줌으로서 구매를 유도한다. 아마존에 접속한 이 사람이 구매가 급한지, 어느 정도까지 지불할 것인지, 또 어떤 상황이고 어떤 성향인지 분석해 각기 다르게 서비스를 제공한다. 이런 기대배송, 예측배송 서비스를 제공하고 있다. 한국은 이와 같은 서비스를 만들기 어려운 이유가 데이터법이 강한 편이다. 개인정보 보호수준이 강하다. 기업들이 고객들의 맥락 정보를 알고 싶으면 다른 회사의 데이터를 가져와야 한다. 앞으로는 가능할 것으로 보이는 것이 최근 데이터 3법이 개정되면서 데이터를 활용할 수 있도록 기회를 주고 있다. 기업들이 가지고 있는 고객 정보를 가명 정보로 만들어 기업들 사이에 거래할 수 있도록 한다. 예를 들어 신혼부부의 상황을 알고 싶은 가전회사들이 청첩장 회사나 허니문 회사, 부동산, 예식장 회사와 콜라보해 데이터를 확보할 수 있을 것이다. 또 어떤 영역에서 초개인화를 활용해 성장할 수 있을까. 금융기업 중에서도 카드회사가 데이터 활용에 힘쓰고 있다. 지금까지 카드사는 카드 추천과 발급을 고객을 그룹화해 해왔다. 미혼, 기혼, 직장인 등의 단순한 분류로서 말이다. 그런데 최근에는 소비자가 1년 동안 어디에 돈을 많이 쓰는 지를 분석해 그 사람에게 필요한 쿠폰을 주는 식으로 개인에게 집중하는 서비스를 개발 중이다. 물론 개인에게 편리해질 초개인화지만 봉이 되지 않기 위해서는 현명한 소비를 해야 할 것이다. 또한 데이터 3법 개정으로 예민하게 반응할 소비자가 있을 것이다. 이 회사를 믿고 내 정보를 준 것이지만 다른 회사에 사용될 수 있기 때문이다. 이런 서비스를 하는 기업들이 본인이 하는 사업의 윤리와 앞으로의 성장 간에 균형을 잘 잡아가야할 것이다. 그리고 그 과정에서 발생되는 문제들을 어떻게 해결해 나가야 할 것인지에 대해 사회적 합의에도 신경 써야 할 것이다. 너무 개인 정보만 보호한다고 기술 투자를 하지 않으면 다른 IT강국에 뒤처질 수 있다. 국가, 기업, 소비자 사이에 합의를 계속해서 해 나가는 과정들이 필요해 보인다.\n00페이 중 가장 대표적인 것이 카카오다. 카톡 서비스로 주식 거래도 가능한 시대가 왔다. 증권업계에서는 굉장히 촉각을 곤두세워야 하는 소식이다. 사용자 입장에서 어떤 플랫폼 안에 새로운 서비스가 들어오면 기존의 경험에 의해 접근성이 떨어지거나 어렵다, 혹은 나와는 상관없는 일이라고 생각 했던 것을 쉽게 할 수 있는 환경이 된다. 예를 들어 요즘 선물 주는 문화가 확 바뀌었다. 메신저 서비스 안에서 선물을 주고받는다. 그리고 그 선물을 주고받는 서비스의 결제가 그 플랫폼의 스마트 페이로 이루어진다. 그 플랫폼 안에 증권이 들어오면 생일 선물로 지금은 커피 쿠폰을 주지만 그 가격만큼 주식을 선물로 줄 수도 있는 것이다. 커피 쿠폰을 선물로 주면 커피를 먹는 것으로 끝나지만 주식으로 주면 몇 년 뒤엔 그 커피 한 잔이 두 잔이 되어 있을 수 있는 일이다. 어느 증권사 서비스에는 이런 것도 있다. 별다방 커피가 4100원인데 5000원을 내고 남은 돈 900원이 그 회사 주식에 들어가는 것이다. 그 커피를 많이 먹을수록 주식이 조금씩 쌓일 것이다. 증권은 나와 상관없는 일이라 생각해왔는데 메신저 플랫폼 안에 들어오게 되고 선물을 하거나 결제를 할 때 티끌모아 태산 같은 주식이 쌓이면 나도 모르게 1년이 지나있으면 해외 주식의 주주가 되어 있을지도 모를 일이다. 카카오가 어떻게 증권에 진출하게 된 것인가. 카카오 뱅크라는 인터넷 은행이 있었는데 하지만 그 인터넷 은행 자체의 지분구조를 보면 다양한 회사들이 있다. 카카오가 1대 주주가 아니기 때문에 카카오 서비스 안에 금융을 붙이고 싶어도 카카오 뱅크로는 마음대로 할 수 없는 상황이었다. 그런 상황에서 1년 전 바로투자증권의 지분 60%를 인수한 뒤 대주주 승인신청을 했는데 1년 만에 승인이 된 것이다. 그러면서 카카오페이 안에 증권사가 들어왔다. 증권사가 들어온다는 것은 CMA 통장에서 금융거래를 할 수 있다는 뜻이다. 페이 서비스 안에서 주식을 사고 선물도 사고 이자도 받고 펀드도 살 수 있게 되는 것이다. ‘핀테크’란 Financial과 Tech를 결합한 것이다. 은행이나 금융기업들이 IT를 활용한 것이다. 요즘은 반대로 카카오나 네이버처럼 기술을 가지고 있는 기업들이 금융에 진출하고 있다. 이런 현상을 **‘테크핀’**이라고 한다. 카카오는 테크핀이 본격화되고 있고, 네이버 역시 준비 중이다.\n가장 큰 신호가 2011년 11월 11일에 있었다. 네이버에서 쇼핑할 때 네이버페이를 쓴다. 이 회사를 아예 Naver Financial로 분산시켰다. 이 쪽에 본격적으로 힘을 싣겠다는 말이다. 12월에 이미 그 회사에서 미래에셋으로부터 8000억 규모의 투자를 받았다. 돈도 있고 결정권도 있으니 올해부터 본격적으로 진출하겠다는 의미다.\n","permalink":"https://HappyJJins.github.io/blog/post2/","tags":["초개인화","빅데이터로 보는 세상","핀테크","테크핀"],"title":"초개인화, 테크핀"},{"categories":["Data analysis _Python"],"contents":"TF-IDF  TF-IDF(Term Frequency – Inverse Document Frequency)는 정보검색론에서 흔하게 접하는 가중치를 구하는 알고리즘 ElasticSearch나 Lucene에서 tf-idf는 검색 결과에 scoring를 하는 데 사용 수식은 다음과 같다.   TF와 IDF에 대해 알아보겠다.\nTF(Term Frequency)  문서 내 특정 단어의 빈도 tf(코로나, 코로나관련기사1)는 관련기사1에서 코로나라는 키워드 빈도수를 뜻한다. 문서 내에 내가 찾는 키워드가 많이 등장한다면 이 문서는 당연히 내 키워드와 관련성이 높다고 할 수 있다. tf값은 단어의 등장 횟수가 증가할수록 무한대로 발산하게 된다. 발산하는 tf값을 정규화시키는 세 가지 방법이 있다. 불린 빈도(Boolean Frequency) 로그 스케일 빈도(Logarithmically Scaled Frequency) 증가 빈도(Augmented Frequency)  불린 빈도  문서에 단어가 나타나면 1, 없으면 0으로 표기 문서에 특정 단어가 1번 나타나든 100번 나타나든 똑같은 가중치를 나타낸다. TF가 중요하지 않은 경우에 사용(단어가 매칭되는 문서가 나타나기만 하면 될 때)  로그 스케일 빈도  로그 변환을 해주면 기울기(증가율)가 처음엔 가파르다가 점점 완만해지는 것을 볼 수 있다. x축이 빈도수, y축이 TF값 작은 빈도수 차이일 땐 TF값의 차가 크지만, 빈도수가 크면 TF값의 차가 그렇게 크지 않다는 것을 볼 수 있다. 즉, 특정 단어가 1번 나타났느냐, 5번 나타났느냐의 차이는 크지만 10000번 나타났느냐  증가 빈도  문서 길이에 따라 단어의 상대적 빈도 값을 조정해주는 방법 어떤 두 문서 특정 단어 빈도수가 동일하다면, 문서길이가 더 짧은 문서에서의 그 단어 빈도수가 상대적으로 더 클 것이다. 스케일이 최대 1로 고정되는 효과 단어의 빈도를 문서 내 단어들의 빈도 중 최대값으로 나눠주는 방법 수식은 다음과 같다.  1차 함수 형태의 그래프로 나타나게 된다.  IDF(Inverse Document Frequency) IDF를 이해하기 위해 DF를 알아보자.\nDF  한 단어가 전체 문서 집합 내에서 얼마나 공통적으로 많이 등장하는지를 나타내는 값 해당 단어가 나타난 문서 수 수식은 다음과 같다.  만약 모든 뉴스기사에 코로나라는 키워드가 많이 등장한다면, 해당 단어 자체가 변별력이 떨어질 것이다. 다른 예로, ‘the’, ‘a’, ‘is’, ‘on’과 같은 모든 문서에 등장할 만한 키워드로 유사성을 계산해 검색결과에 반영한다면 부정확해질 수밖에 없을 것. 특정 단어 t가 모든 문서에 다 등장하는 흔한 단어라면, TF-IDF 가중치를 낮춰주려는 것 DF값이 클수록, TF-IDF의 가중치 값을 낮춰주려면 DF값에 역수를 취해야 한다. DF의 역수가 바로 IDF  IDF  수식은 다음과 같다. 전체 문서의 수가 많을 경우 IDF값이 기하급수적으로 커지게 되므로 로그변환을 해준다.  특정 단어가 전체 문서 안에 존재하지 않을 경우 분모가 0이 되므로 이를 방지하기 위해 분모에 1을 더해주는 것이 일반적.  그래프는 다음과 같다. 특정단어가 들어간 문서가 많을수록, DF가 커질수록 IDF는 낮아진다.   Conclusion  TF-IDF값은 특정 문서 내에 단어 빈도(TF)가 높을수록, 전체 문서들엔 그 단어를 포함한 문서(DF)가 적을수록 높아진다.  Reference (https://thinkwarelab.wordpress.com/2016/11/14/ir-tf-idf-%EC%97%90-%EB%8C%80%ED%95%B4-%EC%95%8C%EC%95%84%EB%B4%85%EC%8B%9C%EB%8B%A4/)\n  ","permalink":"https://HappyJJins.github.io/blog/post1/","tags":["tf-idf","python","data analysis"],"title":"TF-IDF에 대한 쉬운 설명"}]