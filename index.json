[{"categories":["지식 쌓기"],"contents":"감정대리인 ","permalink":"https://HappyJJins.github.io/blog/post5/","tags":["감정대리인","하트시그널","액자형 프로그램","컬링세대","감정의 맥도날드화","감정 큐레이션"],"title":"감정대리인"},{"categories":["Data analysis _Python"],"contents":"Movies Recommender System Package를 설치하는 환경폴더(my_env)를 따로 만들어 실행할 때마다 설치할 필요 없도록 설정했습니다. 이와 관련한 포스팅은 다음에 하겠습니다.\nimport os, sys from google.colab import drive drive.mount(\u0026#39;/content/drive\u0026#39;) my_path = \u0026#39;/content/notebooks\u0026#39; os.symlink(\u0026#39;/content/drive/My Drive/Colab Notebooks/my_env\u0026#39;, my_path) sys.path.insert(0, my_path) !pip install scikit-surprise %matplotlib inline import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from scipy import stats from ast import literal_eval from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer from sklearn.metrics.pairwise import linear_kernel, cosine_similarity from nltk.stem.snowball import SnowballStemmer from nltk.stem.wordnet import WordNetLemmatizer from nltk.corpus import wordnet from surprise import Reader, Dataset, SVD, accuracy from surprise.model_selection import cross_validate from surprise.model_selection import KFold import warnings; warnings.simplefilter(\u0026#39;ignore\u0026#39;) Simple Recommender 영화에 대한 다양한 정보들이 담긴 \u0026lsquo;movies_metadata.csv\u0026rsquo; 파일을 불러와 md 변수에 저장합니다\nmd = pd.read_csv(\u0026#39;/content/drive/My Drive/Colab Notebooks/MovieRcmmd/data/data/movies_metadata.csv\u0026#39;) md.head() \u0026lsquo;genres\u0026rsquo;(장르) 컬럼의 각 행들의 타입이 dict. 장르 id와 이름으로 구성.\n{\u0026lsquo;id\u0026rsquo;: \u0026hellip; , \u0026lsquo;name\u0026rsquo;: \u0026lsquo;\u0026hellip;'},{\u0026lsquo;id\u0026rsquo;: \u0026hellip; ,\u0026lsquo;name\u0026rsquo; : \u0026lsquo;\u0026hellip;'}, ,,,\nmd[\u0026#39;genres\u0026#39;][0] print(type(md[\u0026#39;genres\u0026#39;][0])) # \u0026#34;[{dict}, {dict},] \u0026lt;- dict들이 str으로 묶여있음 md[\u0026#39;genres\u0026#39;].apply(literal_eval) # [{dict},{dict}] \u0026lt;- str을 list로 바꿔줌 #각 영화 장르의 이름을 추출해 list 시켜줌 md[\u0026#39;genres\u0026#39;] = md[\u0026#39;genres\u0026#39;].fillna(\u0026#39;[]\u0026#39;).apply(literal_eval).apply(lambda x : [i[\u0026#39;name\u0026#39;] for i in x] if isinstance(x, list) else []) md[\u0026#39;genres\u0026#39;] # eval(expression, globals=None, locals=none), 내장함수 # expression 인자에 string 값을 넣으면 해당 값을 그대로 실행하여 결과를 출력 expr = \u0026#34;10 + 10\u0026#34; print(type(expr)) print(eval(expr)) #ast.literal_eval(node_or_string), AST module에서 제공하는 함수 #문자 그대로 evaluate 실행하는 함수 import ast str_dict = \u0026#34;{\u0026#39;a\u0026#39;:3,\u0026#39;b\u0026#39;:5}\u0026#34; print(type(str_dict)) convert_dict= ast.literal_eval(str_dict) print(convert_dict[\u0026#39;a\u0026#39;]) print(convert_dict[\u0026#39;b\u0026#39;]) #literal_eval은 ValueError를 발생시킬 수 있다. ast.literal_eval(\u0026#34;10*2\u0026#34;) #literal_eval은 python의 기본 자료형 정도만 evaluate가 가능하도록 지원. eval과 비교해 훨씬 엄격하기 때문에 결과적으로 안전을 보장. type(md[\u0026#39;vote_count\u0026#39;][0]) #vote_count의 타입이 float vote_counts = md[md[\u0026#39;vote_count\u0026#39;].notnull()][\u0026#39;vote_count\u0026#39;].astype(\u0026#39;int\u0026#39;) #float을 int로 vote_averages = md[md[\u0026#39;vote_average\u0026#39;].notnull()][\u0026#39;vote_average\u0026#39;].astype(\u0026#39;int\u0026#39;) C = vote_averages.mean() C m = vote_counts.quantile(0.95) m md[\u0026#39;year\u0026#39;] = pd.to_datetime(md[\u0026#39;release_date\u0026#39;], errors=\u0026#39;coerce\u0026#39;).apply(lambda x : str(x).split(\u0026#39;-\u0026#39;)[0] if x != np.nan else np.nan) # error=\u0026#39;coerce\u0026#39; # 날짜로 된 문자열이 아니라 문자로 된 문자열인 경우 파싱할 수 없다는 ValueError가 발생하는 것을 방지하기 위해 \u0026#39;coerce\u0026#39;옵션 추가 # 파싱할 수 없는 문자열은 \u0026#39;NaN\u0026#39;으로 강제 변환 md[\u0026#39;year\u0026#39;] qualified = md[(md[\u0026#39;vote_count\u0026#39;] \u0026gt;= m) \u0026amp; (md[\u0026#39;vote_count\u0026#39;].notnull()) \u0026amp; (md[\u0026#39;vote_average\u0026#39;].notnull())][[\u0026#39;title\u0026#39;, \u0026#39;year\u0026#39;, \u0026#39;vote_count\u0026#39;, \u0026#39;vote_average\u0026#39;, \u0026#39;popularity\u0026#39;, \u0026#39;genres\u0026#39;]] qualified[\u0026#39;vote_count\u0026#39;] = qualified[\u0026#39;vote_count\u0026#39;].astype(\u0026#39;int\u0026#39;) qualified[\u0026#39;vote_average\u0026#39;] = qualified[\u0026#39;vote_average\u0026#39;].astype(\u0026#39;int\u0026#39;) qualified.shape #(행 수, 열 수) def weighted_rating(x): v = x[\u0026#39;vote_count\u0026#39;] R = x[\u0026#39;vote_average\u0026#39;] return (v/(v+m)*R)+(m/(m+v)*C) # Weighted Rating(WR) # v is the number of votes for the movie # m is the minimum votes required to be listed in the chart # R is the average rating of the movie # C is the mean vote across the whole report qualified[\u0026#39;wr\u0026#39;] = qualified.apply(weighted_rating, axis=1) qualified = qualified.sort_values(\u0026#39;wr\u0026#39;, ascending=False).head(250) qualified[\u0026#39;wr\u0026#39;] qualified.head(15) md.apply(lambda x : pd.Series(x[\u0026#39;genres\u0026#39;]), axis=1) md.apply(lambda x : pd.Series(x[\u0026#39;genres\u0026#39;]), axis=1).stack() s = md.apply(lambda x : pd.Series(x[\u0026#39;genres\u0026#39;]), axis=1).stack().reset_index(level=1, drop=True) # 인덱스 1단계 제거(첫번째 열) s s.name = \u0026#39;genre\u0026#39; gen_md = md.drop(\u0026#39;genres\u0026#39;, axis = 1).join(s) gen_md def build_chart(genre, percentile=0.86): df = gen_md[gen_md[\u0026#39;genre\u0026#39;] == genre] vote_counts = df[df[\u0026#39;vote_count\u0026#39;].notnull()][\u0026#39;vote_count\u0026#39;].astype(\u0026#39;int\u0026#39;) vote_averages = df[df[\u0026#39;vote_average\u0026#39;].notnull()][\u0026#39;vote_average\u0026#39;].astype(\u0026#39;int\u0026#39;) C = vote_averages.mean() m = vote_counts.quantile(percentile) # 여러 개의 칼럼 데이터 추추 시 대괄호 두번[[]] qualified = df[(df[\u0026#39;vote_count\u0026#39;] \u0026gt;= m) \u0026amp; (df[\u0026#39;vote_count\u0026#39;].notnull()) \u0026amp; (df[\u0026#39;vote_average\u0026#39;].notnull())][[\u0026#39;title\u0026#39;, \u0026#39;year\u0026#39;, \u0026#39;vote_count\u0026#39;, \u0026#39;vote_average\u0026#39;, \u0026#39;popularity\u0026#39;]] qualified[\u0026#39;vote_count\u0026#39;] = qualified[\u0026#39;vote_count\u0026#39;].astype(\u0026#39;int\u0026#39;) qualified[\u0026#39;vote_average\u0026#39;] = qualified[\u0026#39;vote_average\u0026#39;].astype(\u0026#39;int\u0026#39;) qualified[\u0026#39;wr\u0026#39;] = qualified.apply(lambda x: (x[\u0026#39;vote_count\u0026#39;]/(x[\u0026#39;vote_count\u0026#39;]+m)*x[\u0026#39;vote_average\u0026#39;])+(m/(m+x[\u0026#39;vote_count\u0026#39;])*C), axis=1) qualified = qualified.sort_values(\u0026#39;wr\u0026#39;, ascending=False).head(250) return qualified Top Romance Movies\nbuild_chart(\u0026#39;Romance\u0026#39;).head(15) links_small = pd.read_csv(\u0026#39;/content/drive/My Drive/Colab Notebooks/MovieRcmmd/data/data/links_small.csv\u0026#39;) links_small # imdbid : internet movie data base # tmdbid : the movie data base links_small = links_small[links_small[\u0026#39;tmdbId\u0026#39;].notnull()][\u0026#39;tmdbId\u0026#39;].astype(\u0026#39;int\u0026#39;) links_small # 여러 개의 행 추출 시 대괄호 두 번 [[]] md.loc[[19730, 29503, 35587]] #id가 날짜 형식 md = md.drop([19730, 29503, 35587]) md[\u0026#39;id\u0026#39;] = md[\u0026#39;id\u0026#39;].astype(\u0026#39;int\u0026#39;) smd = md[md[\u0026#39;id\u0026#39;].isin(links_small)] smd.shape smd.loc[1][\u0026#39;title\u0026#39;] smd.loc[1][\u0026#39;overview\u0026#39;] # overview : 개요 smd.loc[1][\u0026#39;tagline\u0026#39;] # tagline : 광고 구호, 슬로건 smd[\u0026#39;tagline\u0026#39;] = smd[\u0026#39;tagline\u0026#39;].fillna(\u0026#39;\u0026#39;) smd[\u0026#39;description\u0026#39;] = smd[\u0026#39;overview\u0026#39;] + smd[\u0026#39;tagline\u0026#39;] smd[\u0026#39;description\u0026#39;] = smd[\u0026#39;description\u0026#39;].fillna(\u0026#39;\u0026#39;) # tf-idf 인코딩 : 단어 갯수 그대로 카운트하지 않고 모든 문서에 공통적으로 들어있는 단어의 경우 문서 구별 능력이 떨어진다고 보아 가중치를 축소하는 방법 # analyzer=\u0026#39;word\u0026#39;/\u0026#39;char\u0026#39;/\u0026#39;char_wb\u0026#39;(단어 내의 문자) # ngram_range=(min_n, max_n) # min_df=[0.0,1.0]사이의 실수. 디폴트 1, 단어장에 포함되기 위한 최소 빈도 # stop_words=\u0026#39;english\u0026#39; : 영어용 스탑 워드 사용 tf = TfidfVectorizer(analyzer=\u0026#39;word\u0026#39;, ngram_range=(1,2),min_df=0, stop_words=\u0026#39;english\u0026#39;) tfidf_matrix = tf.fit_transform(smd[\u0026#39;description\u0026#39;]) tf.vocabulary_ # 단어를 인덱싱, type은 dict tfidf_matrix.shape # 9099개의 문장이 268124 토큰으로 표현됨 print(np.array(tfidf_matrix)) # (m, n) xxx : 문장 m에서의 단어 n의 tfidf값 cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix) cosine_sim[0] # 1 :첫번째 문장 자기자신과의 코사인 유사도 # 0.00680476 :첫번째 문장과 두 번째 문장과의 코사인 유사도 cosine_sim[9098] smd = smd.reset_index() titles = smd[\u0026#39;title\u0026#39;] indices = pd.Series(smd.index, index=smd[\u0026#39;title\u0026#39;]) smd indices def get_recommendations(title): idx = indices[title] #해당 title 인덱스 sim_scores = list(enumerate(cosine_sim[idx])) #해당 title 과 다른 영화와의 유사도 인덱싱 # (1, 0.00282712634654008) :해당 title과 1번째 영화와의 유사도가 0.002827126346... sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) #유사도를 기준으로 내림차순으로 정렬 sim_scores = sim_scores[1:31] #자기자신과의 유사도(1)을 제외하고, 유사도가 가장 높은 것부터 30개 추출 movie_indices = [i[0] for i in sim_scores] #유사도 상위30개 영화에 대한 인덱스 return titles.iloc[movie_indices] list(enumerate(cosine_sim[indices[\u0026#39;The Godfather\u0026#39;]])) sorted(list(enumerate(cosine_sim[indices[\u0026#39;The Godfather\u0026#39;]])), key=lambda x: x[1], reverse=True) get_recommendations(\u0026#39;The Godfather\u0026#39;).head(10) get_recommendations(\u0026#39;The Dark Knight\u0026#39;).head(10) Metadata Based Recommender\ncredits = pd.read_csv(\u0026#39;/content/drive/My Drive/Colab Notebooks/MovieRcmmd/data/data/credits.csv\u0026#39;) keywords = pd.read_csv(\u0026#39;/content/drive/My Drive/Colab Notebooks/MovieRcmmd/data/data/keywords.csv\u0026#39;) keywords[\u0026#39;id\u0026#39;] = keywords[\u0026#39;id\u0026#39;].astype(\u0026#39;int\u0026#39;) credits[\u0026#39;id\u0026#39;] = credits[\u0026#39;id\u0026#39;].astype(\u0026#39;int\u0026#39;) md[\u0026#39;id\u0026#39;] = md[\u0026#39;id\u0026#39;].astype(\u0026#39;int\u0026#39;) keywords.head() # 각 영화의 keywords 정리 credits.head() # cast :주연들에 대한 설명(character, name, gender,,) # crew : 감독, 작가 등에 대한 설명 md.shape md = md.merge(credits, on=\u0026#39;id\u0026#39;) md = md.merge(keywords, on=\u0026#39;id\u0026#39;) md.head() smd = md[md[\u0026#39;id\u0026#39;].isin(links_small)] smd.shape smd[\u0026#39;cast\u0026#39;] = smd[\u0026#39;cast\u0026#39;].apply(literal_eval) smd[\u0026#39;crew\u0026#39;] = smd[\u0026#39;crew\u0026#39;].apply(literal_eval) smd[\u0026#39;keywords\u0026#39;] = smd[\u0026#39;keywords\u0026#39;].apply(literal_eval) smd[\u0026#39;cast_size\u0026#39;] = smd[\u0026#39;cast\u0026#39;].apply(lambda x: len(x)) smd[\u0026#39;crew_size\u0026#39;] = smd[\u0026#39;crew\u0026#39;].apply(lambda x: len(x)) def get_director(x): for i in x: if i[\u0026#39;job\u0026#39;] == \u0026#39;Director\u0026#39;: return i[\u0026#39;name\u0026#39;] return np.nan smd[\u0026#39;director\u0026#39;] = smd[\u0026#39;crew\u0026#39;].apply(get_director) smd[\u0026#39;cast\u0026#39;] = smd[\u0026#39;cast\u0026#39;].apply(lambda x: [i[\u0026#39;name\u0026#39;] for i in x] if isinstance(x, list) else []) smd[\u0026#39;cast\u0026#39;] = smd[\u0026#39;cast\u0026#39;].apply(lambda x: x[:3] if len(x) \u0026gt;=3 else x) #주연이름 세자리까지만 smd[\u0026#39;keywords\u0026#39;] = smd[\u0026#39;keywords\u0026#39;].apply(lambda x: [i[\u0026#39;name\u0026#39;] for i in x] if isinstance(x, list) else []) smd[\u0026#39;cast\u0026#39;] = smd[\u0026#39;cast\u0026#39;].apply(lambda x: [str.lower(i.replace(\u0026#34; \u0026#34;,\u0026#34;\u0026#34;)) for i in x]) # 주연이름 붙여쓰기, 소문자로 smd[\u0026#39;cast\u0026#39;].head() smd[\u0026#39;director\u0026#39;] = smd[\u0026#39;director\u0026#39;].astype(\u0026#39;str\u0026#39;).apply(lambda x: str.lower(x.replace(\u0026#34; \u0026#34;, \u0026#34;\u0026#34;))) smd[\u0026#39;director\u0026#39;] = smd[\u0026#39;director\u0026#39;].apply(lambda x: [x, x, x]) smd[\u0026#39;director\u0026#39;] Keywords\ns = smd.apply(lambda x: pd.Series(x[\u0026#39;keywords\u0026#39;]), axis=1).stack().reset_index(level=1, drop=True) s.name = \u0026#39;keyword\u0026#39; s s = s.value_counts() s[:5] s = s[s \u0026gt; 1] stemmer = SnowballStemmer(\u0026#39;english\u0026#39;) #접사 제거, 명사 추출 stemmer.stem(\u0026#39;dogs\u0026#39;) def filter_keywords(x): words = [] for i in x: if i in s: words.append(i) return words smd[\u0026#39;keywords\u0026#39;] = smd[\u0026#39;keywords\u0026#39;].apply(filter_keywords) smd[\u0026#39;keywords\u0026#39;] smd[\u0026#39;keywords\u0026#39;] = smd[\u0026#39;keywords\u0026#39;].apply(lambda x: [stemmer.stem(i) for i in x]) smd[\u0026#39;keywords\u0026#39;] = smd[\u0026#39;keywords\u0026#39;].apply(lambda x: [str.lower(i.replace(\u0026#34; \u0026#34;, \u0026#34;\u0026#34;)) for i in x]) smd[\u0026#39;keywords\u0026#39;] smd[\u0026#39;soup\u0026#39;] = smd[\u0026#39;keywords\u0026#39;] + smd[\u0026#39;cast\u0026#39;] + smd[\u0026#39;director\u0026#39;] + smd[\u0026#39;genres\u0026#39;] smd[\u0026#39;soup\u0026#39;] = smd[\u0026#39;soup\u0026#39;].apply(lambda x: \u0026#39; \u0026#39;.join(x)) smd[\u0026#39;soup\u0026#39;] count = CountVectorizer(analyzer=\u0026#39;word\u0026#39;, ngram_range=(1, 2), min_df=0, stop_words=\u0026#39;english\u0026#39;) count_matrix = count.fit_transform(smd[\u0026#39;soup\u0026#39;]) count_matrix.shape # 9212개의 문장이 100304 토큰으로 표현 print(np.array(count_matrix)) cosine_sim = cosine_similarity(count_matrix, count_matrix) cosine_sim smd = smd.reset_index() titles = smd[\u0026#39;title\u0026#39;] indices = pd.Series(smd.index, index=smd[\u0026#39;title\u0026#39;]) indices get_recommendations(\u0026#39;The Dark Knight\u0026#39;).head(10) get_recommendations(\u0026#39;Mean Girls\u0026#39;).head(10) Popularity and Ratings\ndef improved_recommendations(title): idx = indices[title] sim_scores = list(enumerate(cosine_sim[idx])) sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) sim_scores = sim_scores[1:26] movie_indices = [i[0] for i in sim_scores] movies = smd.iloc[movie_indices][[\u0026#39;title\u0026#39;, \u0026#39;vote_count\u0026#39;, \u0026#39;vote_average\u0026#39;, \u0026#39;year\u0026#39;]] vote_counts = movies[movies[\u0026#39;vote_count\u0026#39;].notnull()][\u0026#39;vote_count\u0026#39;].astype(\u0026#39;int\u0026#39;) vote_averages = movies[movies[\u0026#39;vote_average\u0026#39;].notnull()][\u0026#39;vote_average\u0026#39;].astype(\u0026#39;int\u0026#39;) C = vote_averages.mean() m = vote_counts.quantile(0.60) qualified = movies[(movies[\u0026#39;vote_count\u0026#39;] \u0026gt;= m) \u0026amp; (movies[\u0026#39;vote_count\u0026#39;].notnull()) \u0026amp; (movies[\u0026#39;vote_average\u0026#39;].notnull())] qualified[\u0026#39;vote_count\u0026#39;] = qualified[\u0026#39;vote_count\u0026#39;].astype(\u0026#39;int\u0026#39;) qualified[\u0026#39;vote_average\u0026#39;] = qualified[\u0026#39;vote_average\u0026#39;].astype(\u0026#39;int\u0026#39;) qualified[\u0026#39;wr\u0026#39;] = qualified.apply(weighted_rating, axis=1) qualified = qualified.sort_values(\u0026#39;wr\u0026#39;, ascending=False).head(10) return qualified improved_recommendations(\u0026#39;The Dark Knight\u0026#39;) improved_recommendations(\u0026#39;Mean Girls\u0026#39;) Collaborative Filtering\nreader = Reader() ratings = pd.read_csv(\u0026#39;/content/drive/My Drive/Colab Notebooks/MovieRcmmd/data/data/ratings_small.csv\u0026#39;) ratings.head() data = Dataset.load_from_df(ratings[[\u0026#39;userId\u0026#39;, \u0026#39;movieId\u0026#39;, \u0026#39;rating\u0026#39;]], reader) #데이터셋 로딩 svd = SVD() # SVD :특이값 분해 알고리즘 # 데이터를 5개의 부분집합{x1, x2, ,,, , x5}으로 나눈다. # 5개의 부분집합 중 하나의 검증용 데이터셋(test_set)를 제외한 나머지 데이터셋을 학습용 데이터(train_set)로 사용하여 회귀분석 모형을 만들고 test_set으로 검증 # 5회 반복 cross_validate(svd, data, measures=[\u0026#39;RMSE\u0026#39;, \u0026#39;MAE\u0026#39;], cv=5, verbose=True) # kf = KFold(n_splits = 5) # for train_set, test_set in kf.split(data): # svd.fit(train_set) # predictions = svd.test(test_set) # RMSE = accuracy.rmse(predictions, verbose=True) # MAE = accuracy.mae(predictions, verbose=True) print(svd.predict(1, 302)) # 1번 user가 302번 movie에 줬을 점수 추정치가 2.94 Hybrid Recommender\n Input :User ID and the Title of a Movie Output :Similar movies sorted on the basis of expected ratins by that particular user  def convert_int(x): try: return int(x) except: return np.nan id_map = pd.read_csv(\u0026#39;/content/drive/My Drive/Colab Notebooks/MovieRcmmd/data/data/links_small.csv\u0026#39;)[[\u0026#39;movieId\u0026#39;, \u0026#39;tmdbId\u0026#39;]] id_map[\u0026#39;tmdbId\u0026#39;] = id_map[\u0026#39;tmdbId\u0026#39;].apply(convert_int) id_map.columns = [\u0026#39;movieId\u0026#39;, \u0026#39;id\u0026#39;] id_map = id_map.merge(smd[[\u0026#39;title\u0026#39;, \u0026#39;id\u0026#39;]], on=\u0026#39;id\u0026#39;).set_index(\u0026#39;title\u0026#39;) id_mapindices indices_map = id_map.set_index(\u0026#39;id\u0026#39;) indices_map def hybrid(userId, title): idx = indices[title] tmdbId = id_map.loc[title][\u0026#39;id\u0026#39;] movie_id = id_map.loc[title][\u0026#39;movieId\u0026#39;] sim_scores = list(enumerate(cosine_sim[int(idx)])) sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) sim_scores = sim_scores[1:26] movie_indices = [i[0] for i in sim_scores] movies = smd.iloc[movie_indices][[\u0026#39;title\u0026#39;, \u0026#39;vote_count\u0026#39;, \u0026#39;vote_average\u0026#39;, \u0026#39;year\u0026#39;, \u0026#39;id\u0026#39;]] movies[\u0026#39;est\u0026#39;] = movies[\u0026#39;id\u0026#39;].apply(lambda x: svd.predict(userId, indices_map.loc[x][\u0026#39;movieId\u0026#39;]).est) movies = movies.sort_values(\u0026#39;est\u0026#39;, ascending=False) return movies.head(10) hybrid(1, \u0026#39;Avatar\u0026#39;) # 1번 유저가 선택한 영화 \u0026#39;Avatar\u0026#39;와 유사도가 높은 25개의 영화를 추출해 젔을 법한 점수(rating) 추정 # rating est 상위 10개 추출 hybrid(500, \u0026#39;Avatar\u0026#39;) ","permalink":"https://HappyJJins.github.io/blog/post4/","tags":["추천알고리즘","영화추천알고리즘","Python","kaggle"],"title":"영화 추천 알고리즘"},{"categories":["지식 쌓기"],"contents":"IT가 양날의 검이 되는 세상 IT가 양날의 검이 되는 세상이다. 원격근무와 화상회의가 코로나 시대에 좋을 줄만 알았더니 무조건 그렇지도 않다. ‘언텍트’가 더 이상 어색하지 않을 정도로 익숙해졌다. 비대면 서비스가 활성화되고 있다. 그 중에서도 많이 활성화되고 있는 것이 화상회의, 화상교육이다. 코로나19가 길어지다 보니 면접도 화상면접으로 바뀌고 있다. 초기에는 몇몇 IT 기업들만 그런 줄 알았는데 상반기에 공채들을 봤을 때 화상면접으로 많이 바뀌고 있는 추세다. 사람들을 뽑을 때도 언텍트 기술, 화상회의 서비스를 사용하고 있다. 미국에서는 해고도 화상으로 하고 있다. 실리콘 밸리 IT기업에서 상당히 이루어지고 있다. 유동성이 심하다보니 한 번에 대량 해고를 할 때 화상회의 솔루션을 쓰는 것이다. 예를 들어 한 전기 스쿠터 제조사 경우 전체 직원의 30%인 400명을 한 번에 해고했다. 해고 당시 화상회의 솔루션을 활용했다. 이것에 대해 너무하는 것 아니냐는 말에 사장은 이런 얘기를 했다. ‘우리는 조금 더 인간적으로 하려고 비디오는 껐다.’고 말이다. 다른 벤처, 스타트업 같은 경우도 직원을 350명 해고할 때 ‘줌’을 썼다고 한다. 그 CEO는 화상회의를 통해 직원을 해고하는 것이 가장 끔찍한 방식일지도 모르지만 다른 선택이 없었다고 했다. 코로나 때문에 고용시장이 상당히 불안정한 상태다. 이런 식으로 해고하는 기업이 많아질 수밖에 없다는 얘기처럼 들린다. 특히 국내는 고용에 대해 보호받는 부분이 있는데 미국은 계약이나 해고가 우리나라에 비해 자유롭다보니 대규모의 채용이나 해고도 원격으로 진행되고 있다. 그럴 수밖에 없는 이유들 중 하나는 업무 자체의 방식을 완벽하게 언텍트로 바꾸려는 곳도 있다. 지금 아마 우리나라에 몇몇 대학들도 학기 전체를 원격으로 한다. 그만큼이나 회사에 들어갔지만 물리적인 공간을 한 번도 못 가보고 출근과 퇴근도 한 번도 해보지 못하고 채용이 됐다가 잘리게 되는 문화도 생길 수 있다고 볼 수 있다. 화상회의 프로그램이나 앱의 관심이 높아질 수밖에 없고 많이 시작할 것이다. 코로나19로 대부분의 주식들이 하락했고, 경제상황이 좋아지지 않고 있다. 그런데 몇몇 안 되는 홀로 웃는 서비스가 있다. 바로 미국에서 가장 주목 받고 있는 화상회의 솔루션 ‘줌’이다. Z세대가 ‘Zoom’의 Z라는 얘기가 나올 정도로 화제다. 줌을 사용하는 사람들을 지칭하는 ‘Zoomer’라는 신조어도 생겼다. ‘University’ 앞에 ‘Z’를 붙여 ‘줌대학’이라는 용어도 쓰고 있다. 한편 해외 유명대학들이 이 솔루션을 활용하고 있는데 등록금 등의 문제로 학생들의 불만이 많다. 온라인 강의를 들으면서 이렇게 많은 등록금을 내야하는가에 대한 문제다. 최근 주목받는 이유는 직관적이라는 것이다. 다른 서비스들은 그 서비스를 이용하는 사람들은 모두 회원가입을 해 방을 만들어야 한다. 이 서비스는 처음 방을 만드는 사람만 가입해 함께 이용하고 싶은 사람에게 링크를 보내주는데, 그 링크만 있으면 누구나 들어갈 수 있다. 접근성이 상당히 편리하고, 모바일 앱이나 웹을 통해 이용할 수 있다. 최대 100명까지 들어갈 수 있는데, 40분을 무료로 이용할 수 있다. 이로 인해 상당히 재밌는 현상이 벌어지고 있다. 미국 실리콘밸리 화상회의 평균 시간이 30분이라 하는데, 공짜 시간 안에 하려는 것 아닌가 하는 여담이 나오고 있다. 영국의 내각 회의나 우리나라 교육부 회의에서도 이 앱을 활용하고 있다고 한다. 빛이 있으면 그림자가 있는 법이다. 누구나 접근하기 편하다는 장점을 누군가 악용하고 있다. ‘Zoom Booming’이라는 신조어가 생겼다. 줌 폭탄이 떨어진다는 뜻이다. 예를 들어 한 선생님이 방을 만들어 학생들과 화상회의를 하려는데 쉽게 링크를 보내기 위해 SNS에 게재했다. 그런데 학생이 아닌 사람들이 이 링크를 타고 방에 들어가 야한 동영상을 띄우거나 욕을 하고 나가는 것이다. 이런 현상이 실제로 미국에서 일어나고 있다. 이런 경우는 사용자가 방에 비밀번호를 걸어놓지 않은 실수일 경우가 많다. 개인정보를 너무 많이 가져간다는 단점도 있다. 아이폰을 가지고 이 앱을 썼을 때 페이스북이 이 폰에 깔려있지 않는데도 개인정보가 페이스북으로 간다는 것이 발견되기도 했다. 이 문제가 하루 만에 수정되긴 했지만 이런 식으로 예상 외로 개인정보가 다른 곳으로 흘러간다는 것이 이슈화되고 있다. 또한 화상회의 내용이 암호화돼 있지 않아 해킹에 취약하다는 단점도 있다. 줌을 계속 사용하는 것이 불안하다는 내용의 기사를 외신들이 많이 쓰고 있기도 하다. 몇몇 대기업은 보안을 위해 다른 앱으로 옮기고 있다.\n상호작용은 고객과 기업이 만나는 순간을 의미한다. 기업 입장에서는 상호작용은 기회다. 상호작용 시점에 고객의 니즈를 충족시키지 못하면 고객은 다른 방식의 서비스를 찾아가게 될 것이다. ‘Zoom’은 접근성이 편리하다는 점에서 사람들을 끌었지만 ‘보안’을 해결하지 못하면 이용자들은 더 나은 서비스를 찾아 떠나갈 것이다. 줌 이용자가 폭발적으로 늘어나고 있는 지금 이 좋은 기회를 놓치지 말자.\n","permalink":"https://HappyJJins.github.io/blog/post3/","tags":["IT의 양면","빅데이터로 보는 세상","zoom","화상회의","언텍트"],"title":"IT, 양날의 검이 되는 세상"},{"categories":["지식 쌓기"],"contents":" 초개인화 MWC(Mobile World Congress) 취소 카카오톡 서비스로 증권거래도 가능  초개인화 요즘 모임 회비를 입금하거나 단체 회식비를 나눠 낼 때 어떤 방법을 선택하는가. 물론 통장 들고, 카드 들고 은행 달려가던 시절도 있었지만 이젠 스마트폰 하나면 해결되는 시대다. 2007년 스마트폰이 세상에 처음 공개됐으니 불과 13년 전 일인데 그 시절, 아주 먼 옛날 얘기처럼 느껴진다. 이젠 굳이 계좌가 있는 전통 은행 사이트에 접속하지 않고도 스마트폰 버튼만 누르면 송금이 되는 편리한 세상이 됐다. 이렇게 IT 기업 서비스 영역, 빠르게 넓어지고 있다. 2020년 금융시장이 불어올 변화와 혁신의 바람이 기대된다.\n","permalink":"https://HappyJJins.github.io/blog/post2/","tags":["초개인화","빅데이터로 보는 세상","핀테크","테크핀"],"title":"초개인화, 테크핀"},{"categories":["Data analysis _Python"],"contents":"TF-IDF  TF-IDF(Term Frequency – Inverse Document Frequency)는 정보검색론에서 흔하게 접하는 가중치를 구하는 알고리즘 ElasticSearch나 Lucene에서 tf-idf는 검색 결과에 scoring를 하는 데 사용 수식은 다음과 같다.   TF와 IDF에 대해 알아보겠다.\nTF(Term Frequency)  문서 내 특정 단어의 빈도 tf(코로나, 코로나관련기사1)는 관련기사1에서 코로나라는 키워드 빈도수를 뜻한다. 문서 내에 내가 찾는 키워드가 많이 등장한다면 이 문서는 당연히 내 키워드와 관련성이 높다고 할 수 있다. tf값은 단어의 등장 횟수가 증가할수록 무한대로 발산하게 된다. 발산하는 tf값을 정규화시키는 세 가지 방법이 있다. 불린 빈도(Boolean Frequency) 로그 스케일 빈도(Logarithmically Scaled Frequency) 증가 빈도(Augmented Frequency)  불린 빈도  문서에 단어가 나타나면 1, 없으면 0으로 표기 문서에 특정 단어가 1번 나타나든 100번 나타나든 똑같은 가중치를 나타낸다. TF가 중요하지 않은 경우에 사용(단어가 매칭되는 문서가 나타나기만 하면 될 때)  로그 스케일 빈도  로그 변환을 해주면 기울기(증가율)가 처음엔 가파르다가 점점 완만해지는 것을 볼 수 있다. x축이 빈도수, y축이 TF값 작은 빈도수 차이일 땐 TF값의 차가 크지만, 빈도수가 크면 TF값의 차가 그렇게 크지 않다는 것을 볼 수 있다. 즉, 특정 단어가 1번 나타났느냐, 5번 나타났느냐의 차이는 크지만 10000번 나타났느냐  증가 빈도  문서 길이에 따라 단어의 상대적 빈도 값을 조정해주는 방법 어떤 두 문서 특정 단어 빈도수가 동일하다면, 문서길이가 더 짧은 문서에서의 그 단어 빈도수가 상대적으로 더 클 것이다. 스케일이 최대 1로 고정되는 효과 단어의 빈도를 문서 내 단어들의 빈도 중 최대값으로 나눠주는 방법 수식은 다음과 같다.  1차 함수 형태의 그래프로 나타나게 된다.  IDF(Inverse Document Frequency) IDF를 이해하기 위해 DF를 알아보자.\nDF  한 단어가 전체 문서 집합 내에서 얼마나 공통적으로 많이 등장하는지를 나타내는 값 해당 단어가 나타난 문서 수 수식은 다음과 같다.  만약 모든 뉴스기사에 코로나라는 키워드가 많이 등장한다면, 해당 단어 자체가 변별력이 떨어질 것이다. 다른 예로, ‘the’, ‘a’, ‘is’, ‘on’과 같은 모든 문서에 등장할 만한 키워드로 유사성을 계산해 검색결과에 반영한다면 부정확해질 수밖에 없을 것. 특정 단어 t가 모든 문서에 다 등장하는 흔한 단어라면, TF-IDF 가중치를 낮춰주려는 것 DF값이 클수록, TF-IDF의 가중치 값을 낮춰주려면 DF값에 역수를 취해야 한다. DF의 역수가 바로 IDF  IDF  수식은 다음과 같다. 전체 문서의 수가 많을 경우 IDF값이 기하급수적으로 커지게 되므로 로그변환을 해준다.  특정 단어가 전체 문서 안에 존재하지 않을 경우 분모가 0이 되므로 이를 방지하기 위해 분모에 1을 더해주는 것이 일반적.  그래프는 다음과 같다. 특정단어가 들어간 문서가 많을수록, DF가 커질수록 IDF는 낮아진다.   Conclusion  TF-IDF값은 특정 문서 내에 단어 빈도(TF)가 높을수록, 전체 문서들엔 그 단어를 포함한 문서(DF)가 적을수록 높아진다.  Reference (https://thinkwarelab.wordpress.com/2016/11/14/ir-tf-idf-%EC%97%90-%EB%8C%80%ED%95%B4-%EC%95%8C%EC%95%84%EB%B4%85%EC%8B%9C%EB%8B%A4/)\n  ","permalink":"https://HappyJJins.github.io/blog/post1/","tags":["tf-idf","python","data analysis"],"title":"TF-IDF에 대한 쉬운 설명"}]